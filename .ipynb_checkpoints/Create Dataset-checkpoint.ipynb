{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import glob\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from collections import Counter\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = r'/home/protus/Documents/DATASETS/ML_Boot_camp/Colaborative filtering'\n",
    "train_col_dirs = glob.glob(dataset_dir+'/train/date*')\n",
    "SERIALIZED_DIR = './serialized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_run(func_name, *args):\n",
    "    \"\"\"Функция повторного запуска\n",
    "    Позволяет при первом обращении к указанной функции расчитать и сериализовать её ответ,\n",
    "    и при последующих обращениях загружать уже расчитанные сериализованные значения.\n",
    "    \n",
    "    Удобно при работе с функциями требующими длительного времени выполнения\"\"\"\n",
    "    \n",
    "    file = SERIALIZED_DIR+'/'+func_name.__name__+'.pcl'\n",
    "    if os.path.isfile(file):\n",
    "        with open(file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        res = func_name(*args)\n",
    "        if res:\n",
    "            with open(file , 'wb') as f:\n",
    "                pickle.dump(res, f)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = Client(n_workers=4, threads_per_worker=2, processes=False, memory_limit='20GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ds = dd.read_parquet(dataset_dir+'/train/date=2018-02-01/*.parquet', engine='pyarrow')\n",
    "min_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_ds.isna().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dd.read_parquet(dataset_dir+'/train/date*/*.parquet', engine='pyarrow', columns=['instanceId_userId']))\n",
    "print('Размер датасета {0:,}, кол-во столбцов {1}'.format(dataset_size, len(min_ds.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datset = dd.read_parquet(dataset_dir+'/train/date*/*.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#среднее колво Nan на колонку\n",
    "mean_nans = datset.isna().sum(axis=0).mean().compute()/dataset_size*100\n",
    "mean_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#кол-во колонок c пустыми значениями\n",
    "#среднее кол-во Nan в колонке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка типов признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = min_ds.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка вложенных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nested_types = [ isinstance(train_dataset[col][10], np.ndarray) for col in train_dataset.select_dtypes('object').columns]\n",
    "columns_with_nested_types = train_dataset.select_dtypes('object').columns[columns_with_nested_types]\n",
    "columns_with_nested_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['metadata_options'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Описательные статистики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dd_column = lambda column: dd.read_parquet(dataset_dir+'/train/date*/*.parquet', columns=column, engine='pyarrow')\n",
    "get_dd_column_unique = lambda column: get_dd_column([column])[column].unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Кол-во уникальных пользователей {0:,}\\n\\\n",
    "Кол-во уникальных обьектов {1:,}\".format(\\\n",
    "    get_dd_column_unique('instanceId_userId').count(),\n",
    "    get_dd_column_unique('instanceId_objectId').count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#CALC ITEMS EVALUATIONS STATISTICS\n",
    "feedback = get_dd_column(['feedback'])\n",
    "feedback_unique = set()\n",
    "temp = feedback['feedback'].map(lambda x: feedback_unique.update(x)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(feedback)\n",
    "client.cancel(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распределение типов контента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ITEMS TYPES STATISTICS \n",
    "#распределение типов контента\n",
    "instanceId_objectTypes = get_dd_column(['instanceId_objectType']).groupby(by='instanceId_objectType')\\\n",
    "    ['instanceId_objectType'].count().compute()\n",
    "print(\"Типы контента и частоты {0}\".format(instanceId_objectTypes.map(\"{:,}\".format)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_counter(counter):\n",
    "    plt.bar(counter.keys(), counter.values())\n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распределение количества оценок пользователей по контенту Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#выборка по Post типу контента \n",
    "data = get_dd_column(['instanceId_objectType', 'feedback'])\n",
    "post_feedback = data.loc[data.instanceId_objectType=='Post','feedback'].compute()\n",
    "#распределение кол-ва оценок на пользователя\n",
    "f = np.vectorize(lambda x: len(x))\n",
    "post_feedback_distr = Counter(f(post_feedback))\n",
    "print(post_feedback_distr)\n",
    "plot_counter(post_feedback_distr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распределение оценок пользователей c одной оценкой по обьекту по контенту Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def selected_values(x):\n",
    "    if len(x)==1:\n",
    "        return x[0]\n",
    "    \n",
    "f = np.vectorize(selected_values)\n",
    "one_feedback_distr = Counter(f(post_feedback))\n",
    "print(one_feedback_distr)\n",
    "plot_counter(one_feedback_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(post_feedback)\n",
    "post_feedback = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Какое кол-во оценок ставят пользователи?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_values = get_dd_column(['instanceId_userId', 'feedback']).\\\n",
    "                groupby('instanceId_userId')['instanceId_userId'].count().compute()\n",
    "\n",
    "user_values = Counter(user_values)\n",
    "user_values.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в % \n",
    "user_activity = list(zip(*user_values.most_common()))\n",
    "perc = np.array(user_activity[1])/np.array(user_activity[1]).sum()*100\n",
    "plt.plot(perc[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Кол-во оценок на пользователя\n",
    "user_act = list(user_activity[0])\n",
    "user_act.sort()\n",
    "plt.plot(user_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(perc[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Преобладающее большинство пользователей ставят от 1 до 3-х оценок\n",
    "- Более 50% пользователей ставят от 1 до 5 оценок.\n",
    "- Большинство пользователей - 17% ставят 2 оценки\n",
    "- Есть пользователи с аномальным кол-вом оценок больше 1000 -видимо это боты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ активности пользователей по времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime, date\n",
    "\n",
    "user_date_time = get_dd_column(['audit_timestamp', 'instanceId_userId'])\n",
    "user_date_time['audit_timestamp'] = user_date_time['audit_timestamp']\\\n",
    "                .map(lambda x:  datetime.fromtimestamp(x//1000))\n",
    "user_date_time['month_day'] = user_date_time['audit_timestamp']\\\n",
    "                .map(lambda x: str(x.month)+'_'+str(x.day))\n",
    "user_date_time['hour'] = user_date_time['audit_timestamp']\\\n",
    "                .map(lambda x: x.hour)\n",
    "user_date_time['weekday'] = user_date_time['audit_timestamp']\\\n",
    "                .map(lambda x: x.weekday())\n",
    "user_date_time = user_date_time.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "month_day = user_date_time.groupby('month_day')['month_day'].count()\n",
    "month_day.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hours = user_date_time.groupby('hour')['hour'].count()\n",
    "hours.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weekdays = user_date_time.groupby('weekday')['weekday'].count()\n",
    "weekdays.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(user_date_time)\n",
    "user_date_time = None\n",
    "month_day = None\n",
    "hours = None\n",
    "weekdays = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- каких-то существенных аномальных перекосов в пользовательской активности не выявлено\n",
    "- видны недельные циклы пользовательской активности\n",
    "- ожидалось что в даты близкие к 8 марта и 23 февраля могут быть какие нибудь аномальные активности, но эта гипотеза не подтвердилась"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Подготовка тренировочного и валидационного датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка обучающего датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семплирование обучающего датасета\n",
    "- Использовать весь датасет (31млн. строк) для обучения не рентабельно, поэтому просемплируем рандомно небольшую выборку размером с 1,5 млн записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataset_random_sampling(dataset_directory, sample_size=1500000):\n",
    "    \"\"\"Семплирование датасета из parquet файлов\n",
    "    Случайная выборка указанного размера из parquet файлов \n",
    "    \"\"\"\n",
    "    print('Семплирование датасета')\n",
    "    sampled_dataset = df(columns=min_ds.columns)\n",
    "    sample_size = sample_size//len(dataset_directory)\n",
    "\n",
    "    for file in tqdm_notebook(train_col_dirs):\n",
    "        sampled_dataset = pd.concat([sampled_dataset, pq.read_table(file+'/').to_pandas()\\\n",
    "                           .sample(n=sample_size, random_state=10)], ignore_index=True)\n",
    "        \n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разворачиваем вложенные признаки переменной 'metadata_options'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_options_binaryze(data):\n",
    "    metadata_dict = [Counter(x) for x in tqdm_notebook(data['metadata_options'])]\n",
    "    return df(metadata_dict).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Информация о частоте использования контента в процессе ваимодействия пользователя с ресурсом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- В данных содержится информация посещениях пользователя в разное время. \n",
    "- В разные моменты времени кол-во информации о посещенном и оцененном контенте разное. \n",
    "- Поскольку это может сильно влиять на точность модели - то это обстоятельство надо учесть и ввести переменную user_activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем перемнную учета пользовательской активности(кол-ва просмотренных обьектов) на текущий момент \n",
    "def add_user_activity(data):\n",
    "    data['user_activity'] = 1\n",
    "    data['user_activity'] = data.groupby('instanceId_userId')['user_activity'].cumsum()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    #сортируем по времени\n",
    "    data.sort_values(by=['audit_timestamp'], inplace=True)\n",
    "    #добавляем пользовательскую активность\n",
    "    data = add_user_activity(data)\n",
    "    #бинаризация 'metadata_options'\n",
    "    data = pd.concat([data, metadata_options_binaryze(data)], axis=1)\n",
    "    data.drop(['metadata_options'], axis=1,  inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = preprocessing(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Рассчитываем таргет переменную, запоняем пропуски\n",
    "target = train_dataset['feedback'].map(lambda x: 1 if 'Liked' in x else 0).values\n",
    "\n",
    "train_dataset.drop(['feedback'], axis=1, inplace=True)\n",
    "train_dataset.fillna(0, inplace=True, downcast=False)\n",
    "## Проверка\n",
    "train_dataset.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверяем категориальные признаки их типы\n",
    "- те признаки которые являются числами, но являются строками - пытаемся сконвертировать в число"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type_columns = train_dataset.select_dtypes('object').columns\n",
    "converted_columns = []\n",
    "\n",
    "for column in object_type_columns:\n",
    "    try:\n",
    "        train_dataset.loc[:, column] = train_dataset.loc[:, column].astype(int)\n",
    "        converted_columns.append(column)\n",
    "    except Exception as e:\n",
    "        print(column, 'not converted', train_dataset.loc[0, column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делим датасет на TRAIN и VALIDATION с балансировкой по классам\n",
    "- в трейне и тесте должно оказаться приблизительно пропорциональное кол-во класса 1 и 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_dataset, target, test_size=0.30, random_state=42, stratify=target)\n",
    "\n",
    "X_train.reset_index(inplace=True)\n",
    "X_valid.reset_index(inplace=True)\n",
    "\n",
    "X_train = X_train.drop(['index'], axis=1)\n",
    "X_valid = X_valid.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_valid.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сериализуем данные для последующего обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pickle_files(data, name):\n",
    "    with open(name+'.pcl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "pickle_files((X_train, y_train), 'train_dataset')\n",
    "pickle_files((X_valid, y_valid), 'valid_dataset')\n",
    "\n",
    "#убираем ссылки на обьекты для удаления интерпретатором\n",
    "# X_train, y_train, y_valid, train_dataset, target = None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка тестового датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset = pq.read_table(dataset_dir+'/test/').to_pandas(use_threads=8)\n",
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_dataset = preprocessing(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted_columns\n",
    "test_dataset.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#непересекающиеся столбцы в 2-х датасетах\n",
    "set(X_valid.columns)^set(test_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.drop(['date'], axis=1, inplace=True)\n",
    "test_dataset.fillna(0, inplace=True)\n",
    "## Проверка\n",
    "test_dataset.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle_files(test_dataset, 'test_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Проверка пересечений ключевых признаков "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images\n",
    "#objects\n",
    "#texts\n",
    "#users\n",
    "#types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset['instanceId_objectId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset['ImageId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train_dataset['instanceId_objectId'])&set(test_dataset['instanceId_objectId']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ВЫВОДЫ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ввв"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
